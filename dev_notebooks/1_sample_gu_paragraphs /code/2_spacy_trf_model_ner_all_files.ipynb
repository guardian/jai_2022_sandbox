{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0979a1ca-1235-4bba-b7d2-8a803e937901",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extract named entities from downsampled Guardian content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fd5ce65-1210-485f-8eb1-3b9b7dd3782e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luis_flores/.local/share/virtualenvs/nel-mBUv7xk_/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import glob\n",
    "import logging\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c92c4fc-4b18-4984-89db-80aea436475e",
   "metadata": {},
   "source": [
    "# For GPU servers\n",
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f368d9-81ce-437d-85db-3b5b78017678",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "def get_data(doc_index,doc,ent_types):\n",
    "    \"\"\"\n",
    "    Extract the entity data (text, label, start, end, start_char, end_char) \n",
    "    from a Spacy Doc and format into JSON.\n",
    "    Filter output to only include `ent_types`.\n",
    "    :returns dict\n",
    "    \"\"\"\n",
    "    ents = [\n",
    "        {\n",
    "            \"text\": ent.text,\n",
    "            \"label\": ent.label_,\n",
    "            \"start\": ent.start,\n",
    "            \"end\": ent.end,\n",
    "            \"start_char\": ent.start_char,\n",
    "            \"end_char\": ent.end_char,\n",
    "        }\n",
    "        for ent in doc.ents\n",
    "        if ent.label_ in ent_types\n",
    "    ]\n",
    "    return {\"doc_index\":doc_index,\n",
    "            #\"text\": doc.text, \n",
    "            \"ents\": ents}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e902bf6-4f9c-4906-b06d-d86a77151e3f",
   "metadata": {},
   "source": [
    "## Import content from S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404b65b2-5073-46fc-849e-2077c41c6d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"eu-west-1\"\n",
    "SESSION = boto3.Session(region_name=REGION)\n",
    "\n",
    "def list_models_on_s3(bucket, path, session, endpoint_url=None):\n",
    "    s3 = session.resource(\"s3\", endpoint_url=endpoint_url)\n",
    "    my_bucket = s3.Bucket(bucket)\n",
    "    bucket_contents = []\n",
    "    for my_bucket_object in my_bucket.objects.filter(Prefix=path):\n",
    "        if not my_bucket_object.key.endswith(\"/\"):\n",
    "            bucket_contents.append(my_bucket_object.key)\n",
    "    return bucket_contents\n",
    "\n",
    "\n",
    "def load_files_from_s3(bucket, session, file_list, destination, endpoint_url=None):\n",
    "    s3 = session.resource(\"s3\", endpoint_url=endpoint_url)\n",
    "    my_bucket = s3.Bucket(bucket)\n",
    "    for file in file_list:\n",
    "        if '2022' in file:\n",
    "            my_bucket.download_file(file, destination + file.split(\"/\")[-1])\n",
    "    return destination\n",
    "\n",
    "bucket='jai-datasets'\n",
    "path='GU_sample_data'\n",
    "session=SESSION\n",
    "list_models_on_s3(bucket,path,session)\n",
    "\n",
    "bucket='jai-datasets'\n",
    "path='GU_sample_data'\n",
    "session=SESSION\n",
    "file_list=list_models_on_s3(bucket,path,session)\n",
    "destination = '../assets/'\n",
    "load_files_from_s3(bucket, session, file_list, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c7ea9b-3f27-4981-aea9-43ddabf1d1fc",
   "metadata": {},
   "source": [
    "## Use spaCy's en_core_web_trf NER model to extract entities from content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5baaa9-9e06-4d89-85de-b2ea5bc41680",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('Starting NER extraction')\n",
    "\n",
    "logging.info('Loading Spacy model')\n",
    "NER_TRF_MODEL=\"en_core_web_trf\"\n",
    "nlp = spacy.load(NER_TRF_MODEL)\n",
    "ent_types = nlp.pipe_labels[\"ner\"]\n",
    "unwanted_ent_types=['CARDINAL','LANGUAGE','ORDINAL','PERCENT','QUANTITY','TIME']\n",
    "ent_types = [ent for ent in ent_types if ent not in unwanted_ent_types]\n",
    "csv_file_list=glob.glob('../assets/*.csv')\n",
    "csv_file_list.sort()\n",
    "#start on most recent year\n",
    "csv_file_list.reverse()\n",
    "\n",
    "logging.info('Starting iteration through csv files')\n",
    "for csv_file in csv_file_list:\n",
    "    csv_file_name=''.join(csv_file.split('/')[-1].split('.')[-2])\n",
    "    export_csv_file=f'../assets/{csv_file_name}_ner.csv.gz'\n",
    "    if glob.glob(export_csv_file):\n",
    "        # Stop entity extraction for files already processed\n",
    "        continue\n",
    "    logging.info('------------------------')\n",
    "    logging.info(f'Reading {csv_file} data')\n",
    "    try:\n",
    "        data=pd.read_csv(csv_file)\n",
    "    except:\n",
    "        continue\n",
    "    data['body_text']=data['body_text'].astype('str')\n",
    "    # Ensure incremental ordered index to reference back to articles in the dataset\n",
    "    data=data.reset_index(drop=True).sort_index()\n",
    "    data=data.to_dict('index')\n",
    "    gu_article_list=[data[key]['body_text'] for key in data.keys()]\n",
    "    response_body = []\n",
    "    exceptions=[]\n",
    "    for doc_index,doc in enumerate(nlp.pipe(gu_article_list, batch_size=20)):\n",
    "        if doc_index%1000==0:\n",
    "            logging.info(f'Extracting named entities from {csv_file} article {doc_index}')\n",
    "        response_body.append(get_data(doc_index,doc, ent_types))\n",
    "    d={}\n",
    "    i=-1\n",
    "    for response in response_body:\n",
    "        for ent_ind,ent in enumerate(response['ents']):\n",
    "            i+=1\n",
    "            ent['doc_index']=response['doc_index']\n",
    "            d[i]=ent\n",
    "    df=pd.DataFrame.from_dict(d,orient='index')\n",
    "    df.to_csv(export_csv_file)\n",
    "    logging.info(f'Finished processing {csv_file}')\n",
    "    logging.info('------------------------')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "43dff093-0ed7-4388-86ad-7808d18837ce",
   "metadata": {},
   "source": [
    "NB: Following iterations could use the Guardian PERSON NER model to improve the named entitiy extraction quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

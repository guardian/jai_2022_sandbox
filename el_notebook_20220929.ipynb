{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Entity Linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entity Linking** (EL) is the challenge of resolving ambiguous textual mentions to unique concepts in a knowledge base. A related task is **Named Entity Recognition** (NER). An NER component basically identifies words in text that have a specific name and refer to real-world objects, such as people or organizations. spaCy offers pre-built Machine Learning models that perform Named Entity Recognition for a variety of languages (https://spacy.io/models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "!pip install spacy==3.0.6\n",
    "!pip install spacy-lookups-data\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entity 'Emerson' with label 'PERSON'\n",
      "Named Entity 'Wimbledon' with label 'EVENT'\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "text = \"Tennis champion Emerson was expected to win Wimbledon.\"\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(f\"Named Entity '{ent.text}' with label '{ent.label_}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Knowledge Base "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to perform Entity Linking, is to set up a knowledge base that contains the unique identifiers of the entities we are interested in. In this tutorial we will create a very simple one with only 3 entries. We load the data from a pre-defined CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='full'# OR'open_sanctions'# OR 'lilsis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import kb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luis_flores/.local/share/virtualenvs/ner-DhZLIlym/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3553: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv(f'kb_datasets/kb_entities_{dataset}.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_index</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>desc</th>\n",
       "      <th>AKA</th>\n",
       "      <th>kb_origin</th>\n",
       "      <th>kb_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>acf-00040861bc3f593000830d987d09967ef3503ef1</td>\n",
       "      <td>kolyvanov egor</td>\n",
       "      <td>Russian host of news program on NTV Federal ma...</td>\n",
       "      <td>['Kolyvanov Egor', 'Колыванов Егор']</td>\n",
       "      <td>open_sanctions</td>\n",
       "      <td>https://www.opensanctions.org/entities/acf-000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>acf-0011c68a768924609dc5da5707ac7fa4c4d645a2</td>\n",
       "      <td>shipov sergei yurievich</td>\n",
       "      <td>Russian chess chess Publicly supported war aga...</td>\n",
       "      <td>['Shipov Sergei Yurievich', 'Шипов Сергей Юрье...</td>\n",
       "      <td>open_sanctions</td>\n",
       "      <td>https://www.opensanctions.org/entities/acf-001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>acf-001e7e4c0363f08f1e784c230457960b84a6416f</td>\n",
       "      <td>egorov ivan mikhailovich</td>\n",
       "      <td>Deputy of the State Council of the Republic of...</td>\n",
       "      <td>['Egorov Ivan Mikhailovich', 'Егоров Иван Миха...</td>\n",
       "      <td>open_sanctions</td>\n",
       "      <td>https://www.opensanctions.org/entities/acf-001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>acf-002c208139012c8d93b6298358188d7cadafe648</td>\n",
       "      <td>goreslavsky alexey sergeyevich</td>\n",
       "      <td>Russian journalist and media Helped destroy in...</td>\n",
       "      <td>['Goreslavsky Alexey Sergeyevich', 'Гореславск...</td>\n",
       "      <td>open_sanctions</td>\n",
       "      <td>https://www.opensanctions.org/entities/acf-002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>acf-002cc8fdf8fe41185091a7cb6c598663e7a22eb5</td>\n",
       "      <td>samoilova natalya vladimirovna</td>\n",
       "      <td>Russian Supported the actions of the Russian m...</td>\n",
       "      <td>['Samoilova Natalya Vladimirovna', 'Самойлова ...</td>\n",
       "      <td>open_sanctions</td>\n",
       "      <td>https://www.opensanctions.org/entities/acf-002...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original_index                                            id  \\\n",
       "0               0  acf-00040861bc3f593000830d987d09967ef3503ef1   \n",
       "1               1  acf-0011c68a768924609dc5da5707ac7fa4c4d645a2   \n",
       "2               2  acf-001e7e4c0363f08f1e784c230457960b84a6416f   \n",
       "3               3  acf-002c208139012c8d93b6298358188d7cadafe648   \n",
       "4               4  acf-002cc8fdf8fe41185091a7cb6c598663e7a22eb5   \n",
       "\n",
       "                             name  \\\n",
       "0                  kolyvanov egor   \n",
       "1         shipov sergei yurievich   \n",
       "2        egorov ivan mikhailovich   \n",
       "3  goreslavsky alexey sergeyevich   \n",
       "4  samoilova natalya vladimirovna   \n",
       "\n",
       "                                                desc  \\\n",
       "0  Russian host of news program on NTV Federal ma...   \n",
       "1  Russian chess chess Publicly supported war aga...   \n",
       "2  Deputy of the State Council of the Republic of...   \n",
       "3  Russian journalist and media Helped destroy in...   \n",
       "4  Russian Supported the actions of the Russian m...   \n",
       "\n",
       "                                                 AKA       kb_origin  \\\n",
       "0               ['Kolyvanov Egor', 'Колыванов Егор']  open_sanctions   \n",
       "1  ['Shipov Sergei Yurievich', 'Шипов Сергей Юрье...  open_sanctions   \n",
       "2  ['Egorov Ivan Mikhailovich', 'Егоров Иван Миха...  open_sanctions   \n",
       "3  ['Goreslavsky Alexey Sergeyevich', 'Гореславск...  open_sanctions   \n",
       "4  ['Samoilova Natalya Vladimirovna', 'Самойлова ...  open_sanctions   \n",
       "\n",
       "                                              kb_url  \n",
       "0  https://www.opensanctions.org/entities/acf-000...  \n",
       "1  https://www.opensanctions.org/entities/acf-001...  \n",
       "2  https://www.opensanctions.org/entities/acf-001...  \n",
       "3  https://www.opensanctions.org/entities/acf-002...  \n",
       "4  https://www.opensanctions.org/entities/acf-002...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_data=data[['id','name','desc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360277, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate synthetic aliases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "aliases_data=kb_data[kb_data['name'].duplicated(keep=False)].sort_values(['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "aliases_data['id']=aliases_data['id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alias_dict={}\n",
    "for alias in aliases_data['name'].unique():\n",
    "    alias_dict[alias]=list(aliases_data.loc[aliases_data['name']==alias, 'id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "david smith                  13\n",
       "david wilson                 13\n",
       "mark smith                   13\n",
       "john williams                12\n",
       "david anderson               11\n",
       "                             ..\n",
       "christian grobet              1\n",
       "pedro maría corral corral     1\n",
       "christian franqueville        1\n",
       "christian favier              1\n",
       "zhu haowen                    1\n",
       "Name: name, Length: 346764, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb_data['name'].value_counts()#.to_csv('value_counts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>name</th>\n",
       "      <th>desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acf-00040861bc3f593000830d987d09967ef3503ef1</td>\n",
       "      <td>kolyvanov egor</td>\n",
       "      <td>Russian host of news program on NTV Federal ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acf-0011c68a768924609dc5da5707ac7fa4c4d645a2</td>\n",
       "      <td>shipov sergei yurievich</td>\n",
       "      <td>Russian chess chess Publicly supported war aga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acf-001e7e4c0363f08f1e784c230457960b84a6416f</td>\n",
       "      <td>egorov ivan mikhailovich</td>\n",
       "      <td>Deputy of the State Council of the Republic of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acf-002c208139012c8d93b6298358188d7cadafe648</td>\n",
       "      <td>goreslavsky alexey sergeyevich</td>\n",
       "      <td>Russian journalist and media Helped destroy in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acf-002cc8fdf8fe41185091a7cb6c598663e7a22eb5</td>\n",
       "      <td>samoilova natalya vladimirovna</td>\n",
       "      <td>Russian Supported the actions of the Russian m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131277</th>\n",
       "      <td>Q6959189</td>\n",
       "      <td>nahapet gevorgyan</td>\n",
       "      <td>Armenian politician Armenians the Assembly Arm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164305</th>\n",
       "      <td>Q7281459</td>\n",
       "      <td>radnaasumbereliyn gonchigdorj</td>\n",
       "      <td>Mongolian politician the Assembly Province Ark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125067</th>\n",
       "      <td>Q9131042</td>\n",
       "      <td>nyamaagiin enkhbold</td>\n",
       "      <td>Mongolian politician the Assembly Uliastai Def...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144084</th>\n",
       "      <td>Q9190484</td>\n",
       "      <td>d. o. chaoke</td>\n",
       "      <td>Linguist of Tungusic languages Foreign Studies...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179437</th>\n",
       "      <td>Q9203344</td>\n",
       "      <td>zhu haowen</td>\n",
       "      <td>Mayor of China Zhu Congress University male Ch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360277 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 qid  \\\n",
       "0       acf-00040861bc3f593000830d987d09967ef3503ef1   \n",
       "1       acf-0011c68a768924609dc5da5707ac7fa4c4d645a2   \n",
       "2       acf-001e7e4c0363f08f1e784c230457960b84a6416f   \n",
       "3       acf-002c208139012c8d93b6298358188d7cadafe648   \n",
       "4       acf-002cc8fdf8fe41185091a7cb6c598663e7a22eb5   \n",
       "...                                              ...   \n",
       "131277                                      Q6959189   \n",
       "164305                                      Q7281459   \n",
       "125067                                      Q9131042   \n",
       "144084                                      Q9190484   \n",
       "179437                                      Q9203344   \n",
       "\n",
       "                                  name  \\\n",
       "0                       kolyvanov egor   \n",
       "1              shipov sergei yurievich   \n",
       "2             egorov ivan mikhailovich   \n",
       "3       goreslavsky alexey sergeyevich   \n",
       "4       samoilova natalya vladimirovna   \n",
       "...                                ...   \n",
       "131277               nahapet gevorgyan   \n",
       "164305   radnaasumbereliyn gonchigdorj   \n",
       "125067             nyamaagiin enkhbold   \n",
       "144084                    d. o. chaoke   \n",
       "179437                      zhu haowen   \n",
       "\n",
       "                                                     desc  \n",
       "0       Russian host of news program on NTV Federal ma...  \n",
       "1       Russian chess chess Publicly supported war aga...  \n",
       "2       Deputy of the State Council of the Republic of...  \n",
       "3       Russian journalist and media Helped destroy in...  \n",
       "4       Russian Supported the actions of the Russian m...  \n",
       "...                                                   ...  \n",
       "131277  Armenian politician Armenians the Assembly Arm...  \n",
       "164305  Mongolian politician the Assembly Province Ark...  \n",
       "125067  Mongolian politician the Assembly Uliastai Def...  \n",
       "144084  Linguist of Tungusic languages Foreign Studies...  \n",
       "179437  Mayor of China Zhu Congress University male Ch...  \n",
       "\n",
       "[360277 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export kb data in right format for tutorialkb_data\n",
    "kb_data.rename(columns={'id':'qid','context':'desc'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csv\n",
    "from pathlib import Path\n",
    "\n",
    "def load_entities(kb_data):\n",
    "    #entities_loc = Path.cwd()/f'kb_datasets/kb_entities_{dataset}.csv'\n",
    "    #kb_entities=pd.read_csv(entities_loc, names=['qid','name','desc'])\n",
    "\n",
    "    names = dict()\n",
    "    descriptions = dict()\n",
    "\n",
    "    for row in kb_data.iterrows():\n",
    "        qid = str(row[1][0])\n",
    "        name = str(row[1][1])\n",
    "        desc = str(row[1][2])\n",
    "        names[qid] = name\n",
    "        descriptions[qid] = desc\n",
    "    \n",
    "    return names, descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name_dict, desc_dict = load_entities(kb_data)\n",
    "for QID in name_dict.keys():\n",
    "    print(f\"{QID}, name={name_dict[QID]}, desc={desc_dict[QID]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.kb import KnowledgeBase\n",
    "kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add each record to the knowledge base, we encode its description using the built-in word vectors of our `nlp` model. The `vector` attribute of a document is the average of its token vectors. We also need to provide a frequency, which is a raw count of how many times a certain entity appears in an annotated corpus. In this tutorial we're not using these frequencies, so we're setting them to an arbitrary value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "descriptions_enc = dict()\n",
    "for qid, desc in desc_dict.items():\n",
    "    desc_doc = nlp(desc)\n",
    "    desc_enc = desc_doc.vector\n",
    "    descriptions_enc[qid]=desc_enc\n",
    "    kb.add_entity(entity=qid, entity_vector=desc_enc, freq=342)   # 342 is an arbitrary value here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('kb_datasets/descriptions_enc.pickle', 'wb') as handle:\n",
    "    pickle.dump(descriptions_enc, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to specify aliases or synonyms. We first add the full names. Here, we are 100% certain that they resolve to their corresponding QID, as there is no ambiguity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qid, name in name_dict.items():\n",
    "    if name not in alias_dict.keys():\n",
    "        kb.add_alias(alias=str(name), entities=[str(qid)], probabilities=[1])   # 100% prior probability P(entity|alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alias_ in alias_dict.keys():\n",
    "    qids=alias_dict[alias_]\n",
    "    probs = [round(1/len(qids),2)-.01 for qid in qids]\n",
    "    kb.add_alias(alias=alias_, entities=qids, probabilities=probs)  # sum([probs]) should be <= 1 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to add the alias \"Emerson\". We'll assume that each of our 3 Emersons is equally famous and thus we set their probabilities to be equal for each entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this will be our Knowledge base. We can check the entities and aliases that are contained in it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the candidates that are generated for the full name of Roy Emerson, as well as for the mention \"Emerson\" or for any other random mention, like \"Sofie\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates for joe biden: ['13047', 'Q6279']\n"
     ]
    }
   ],
   "source": [
    "candidate_1='joe biden'\n",
    "print(f\"Candidates for {candidate_1}: {[c.entity_ for c in kb.get_alias_candidates(candidate_1)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates for adam smith: ['269916', 'Q350916', '379819', '256328', '129552', '13596']\n"
     ]
    }
   ],
   "source": [
    "candidate_2='adam smith'\n",
    "print(f\"Candidates for {candidate_2}: {[c.entity_ for c in kb.get_alias_candidates(candidate_2)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates for david smith: ['Q53960880', '280783', '184041', '377020', '211703', 'Q5239878', '53881', '200407', '77215', '200405', '204251', '221595', 'Q3018800']\n"
     ]
    }
   ],
   "source": [
    "candidate_3='david smith'\n",
    "print(f\"Candidates for {candidate_3}: {[c.entity_ for c in kb.get_alias_candidates(candidate_3)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that querying the KB with the alias \"Emerson\" gives us 3 candidates, but if we query it with an unknown term, it just gives an empty list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the knowledge base by calling the function `to_disk` with an output location."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "f = open(\"source_data.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "source_data_sentences=[item + ' \\n' for sublist in [article.split('.') for article in list(f)] for item in sublist] "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(source_data_sentences)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('source_data_sentences.txt', 'w') as fp:\n",
    "    for item in source_data_sentences:\n",
    "        # write each item on a new line\n",
    "        fp.write(item)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# change the directory and file names to whatever you like\n",
    "import os\n",
    "output_dir = Path.cwd() / \"my_output\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir) \n",
    "kb.to_disk(output_dir / f'kb_{dataset}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can store the `nlp` object to file by calling `to_disk` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(output_dir / f'nlp_{dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120424</th>\n",
       "      <td>Q3018800</td>\n",
       "      <td>david smith</td>\n",
       "      <td>Quebec politician the member David Commons Smi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144574</th>\n",
       "      <td>Q5239878</td>\n",
       "      <td>david smith</td>\n",
       "      <td>Canadian senator Law of male University House ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145959</th>\n",
       "      <td>Q53960880</td>\n",
       "      <td>david smith</td>\n",
       "      <td>Australian Capital Territory politician Repres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221305</th>\n",
       "      <td>53881</td>\n",
       "      <td>david smith</td>\n",
       "      <td>Prospective Parliamentary Candidate for Wakefield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230837</th>\n",
       "      <td>77215</td>\n",
       "      <td>david smith</td>\n",
       "      <td>Dasa Properties LLC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277443</th>\n",
       "      <td>184041</td>\n",
       "      <td>david smith</td>\n",
       "      <td>Sun Coast Resources Inc Humble Texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285064</th>\n",
       "      <td>200405</td>\n",
       "      <td>david smith</td>\n",
       "      <td>University of Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285066</th>\n",
       "      <td>200407</td>\n",
       "      <td>david smith</td>\n",
       "      <td>San Bernardino Community College District</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287175</th>\n",
       "      <td>204251</td>\n",
       "      <td>david smith</td>\n",
       "      <td>San Bernardino Community College District</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291558</th>\n",
       "      <td>211703</td>\n",
       "      <td>david smith</td>\n",
       "      <td>Property Management Pennsylvania Turnpike Comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296598</th>\n",
       "      <td>221595</td>\n",
       "      <td>david smith</td>\n",
       "      <td>Regional Vice Anthem Blue Cross Blue Shield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321458</th>\n",
       "      <td>280783</td>\n",
       "      <td>david smith</td>\n",
       "      <td>Chief Development Leavitt Partners</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344698</th>\n",
       "      <td>377020</td>\n",
       "      <td>david smith</td>\n",
       "      <td>PNC Bank</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id         name  \\\n",
       "120424   Q3018800  david smith   \n",
       "144574   Q5239878  david smith   \n",
       "145959  Q53960880  david smith   \n",
       "221305      53881  david smith   \n",
       "230837      77215  david smith   \n",
       "277443     184041  david smith   \n",
       "285064     200405  david smith   \n",
       "285066     200407  david smith   \n",
       "287175     204251  david smith   \n",
       "291558     211703  david smith   \n",
       "296598     221595  david smith   \n",
       "321458     280783  david smith   \n",
       "344698     377020  david smith   \n",
       "\n",
       "                                                     desc  \n",
       "120424  Quebec politician the member David Commons Smi...  \n",
       "144574  Canadian senator Law of male University House ...  \n",
       "145959  Australian Capital Territory politician Repres...  \n",
       "221305  Prospective Parliamentary Candidate for Wakefield  \n",
       "230837                                Dasa Properties LLC  \n",
       "277443               Sun Coast Resources Inc Humble Texas  \n",
       "285064                              University of Florida  \n",
       "285066          San Bernardino Community College District  \n",
       "287175          San Bernardino Community College District  \n",
       "291558  Property Management Pennsylvania Turnpike Comm...  \n",
       "296598        Regional Vice Anthem Blue Cross Blue Shield  \n",
       "321458                 Chief Development Leavitt Partners  \n",
       "344698                                           PNC Bank  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb_data[kb_data['name']=='david smith']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export guardian articles as random txt sentences"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "#gu_source_data='entity_source_data/gu_resampled_by_section_id.csv'\n",
    "#gu_sample=pd.read_csv(gu_source_data,index_col=0)\n",
    "#gu_sample['paragraphs'] = gu_sample['body_html'].apply(get_article_paragraphs)\n",
    "#gu_sample['paragraphs'] = gu_sample['body_html'].apply(get_article_paragraphs)\n",
    "#gu_sample['paragraphs']=gu_sample['paragraphs'].apply(lambda x: '<p>'.join(x))\n",
    "#gu_sample=gu_sample['paragraphs'].str.split('<p>').explode().to_frame()\n",
    "article_containing_alias_indices=[]\n",
    "for alias in kb_entities['name'].unique():\n",
    "    if not gu_sample.loc[gu_sample['paragraphs'].str.contains(alias),'paragraphs'].empty:\n",
    "        article_containing_alias_indices.append(gu_sample[gu_sample['paragraphs'].str.contains(alias)])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gu_sample.reset_index().to_csv('entity_source_data/gu_resampled_by_section_id_sentences_with_full_matches.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gu_sample=pd.read_csv('entity_source_data/gu_resampled_by_section_id_sentences_with_full_matches.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gu_sample=gu_sample.sample(frac=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gu_sample.drop(['index'],1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "output_name='gu_resampled_by_section_id_sentences_with_full_matches'\n",
    "article_containing_alias=gu_sample\n",
    "articles_containing_alias=[df_row[['body_text']].values[0][0] for df_row in article_containing_alias_indices]\n",
    "\n",
    "with open(Path.cwd() / 'entity_source_data' / f'{output_name}.txt', 'w') as fp:\n",
    "    for item in articles_containing_alias:\n",
    "        fp.write(\"%s\\n\" % item)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>112839</th>\n",
       "      <td>[ And given Democrats’ extremely narrow majori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4849</th>\n",
       "      <td>[ Then Edward Heath suspended Stormont]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127342</th>\n",
       "      <td>[ “I’m my own boss]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107479</th>\n",
       "      <td>[ “After how scary the last year has been, we’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85172</th>\n",
       "      <td>[ But it has not been able to persuade the EU ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4611</th>\n",
       "      <td>[ “To get that message out five days in advanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131659</th>\n",
       "      <td>[ Yellen says this is important]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50094</th>\n",
       "      <td>[ A Labour source has been in touch to say tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65216</th>\n",
       "      <td>[ A USCIS spokeswoman, Pamela Wilson, said the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166282</th>\n",
       "      <td>[ Elizabeth Collett, the director of the Migra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2360186 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body_text\n",
       "112839  [ And given Democrats’ extremely narrow majori...\n",
       "4849              [ Then Edward Heath suspended Stormont]\n",
       "127342                                [ “I’m my own boss]\n",
       "107479  [ “After how scary the last year has been, we’...\n",
       "85172   [ But it has not been able to persuade the EU ...\n",
       "...                                                   ...\n",
       "4611    [ “To get that message out five days in advanc...\n",
       "131659                   [ Yellen says this is important]\n",
       "50094   [ A Labour source has been in touch to say tha...\n",
       "65216   [ A USCIS spokeswoman, Pamela Wilson, said the...\n",
       "166282  [ Elizabeth Collett, the director of the Migra...\n",
       "\n",
       "[2360186 rows x 1 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_containing_alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "gu_sample['body_text']=gu_sample['body_text'].apply(lambda r: r.split('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "gu_sample=gu_sample['body_text'].explode().sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "with open(Path.cwd() /f'entity_source_data/{output_name}_resampled_sentences_randomised.txt', 'w') as fp:\n",
    "    for item in gu_sample:\n",
    "        fp.write(\"%s.\\n\" % item)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to create some annotated data to train an Entity Linking algorithm on. To do so, we will use the annotation tool Prodigy, but you could generate the data in whatever tool you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are watching [the video](https://www.youtube.com/watch?v=8u57WSXVpmw), it will explain how to obtain annotated data with Prodigy. The final result will be a JSONL file that is distributed alongside this notebook. We'll now use this JSONL file to train our entity linker. If you want to skip the annotation part in the video, you can fast forward to [this section](https://www.youtube.com/watch?v=8u57WSXVpmw&t=19m19s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the results in this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "json_loc = Path.cwd().parent / \"assets\" / \"emerson_annotated_text.jsonl\" # distributed alongside this notebook\n",
    "with json_loc.open(\"r\", encoding=\"utf8\") as jsonfile:\n",
    "    line = jsonfile.readline()\n",
    "    print(line)   # print just the first line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We see that the full text of the original sentence is stored, together with a lot of detail about the annotation task. The most important bit is stored with the key `accept` at the end: this is the value of our manual annotation. For this specific sentence and this specific mention, the option with key `Q312545` was manually selected. This is the information that we'll train our entity linker on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Entity Linker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed training data into our Entity Linker, we format our data as a structured tuple. The first part is the raw text, and the second part is a dictionary of annotations. This dictionary defines the named entities we want to link (\"entities\"), as well as the actual gold-standard links (\"links\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "dataset = []\n",
    "json_loc = Path.cwd().parent / \"assets\" / \"emerson_annotated_text.jsonl\"\n",
    "with json_loc.open(\"r\", encoding=\"utf8\") as jsonfile:\n",
    "    for line in jsonfile:\n",
    "        example = json.loads(line)\n",
    "        text = example[\"text\"]\n",
    "        if example[\"answer\"] == \"accept\":\n",
    "            QID = example[\"accept\"][0]\n",
    "            offset = (example[\"spans\"][0][\"start\"], example[\"spans\"][0][\"end\"])\n",
    "            entity_label = example[\"spans\"][0][\"label\"]\n",
    "            entities = [(offset[0], offset[1], entity_label)]\n",
    "            links_dict = {QID: 1.0}\n",
    "        dataset.append((text, {\"links\": {offset: links_dict}, \"entities\": entities}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check whether the conversion looks OK, we can just print the first sample in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check some statistics in this dataset. How many cases of each QID do we have annotated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_ids = []\n",
    "for text, annot in dataset:\n",
    "    for span, links_dict in annot[\"links\"].items():\n",
    "        for link, value in links_dict.items():\n",
    "            if value:\n",
    "                gold_ids.append(link)\n",
    "\n",
    "from collections import Counter\n",
    "print(Counter(gold_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got exactly 10 annotated sentences for each of our Emersons. Of these, we'll now set aside 6 cases in a separate test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_dataset = []\n",
    "test_dataset = []\n",
    "for QID in qids:\n",
    "    indices = [i for i, j in enumerate(gold_ids) if j == QID]\n",
    "    train_dataset.extend(dataset[index] for index in indices[0:8])  # first 8 in training\n",
    "    test_dataset.extend(dataset[index] for index in indices[8:10])  # last 2 in test\n",
    "    \n",
    "random.shuffle(train_dataset)\n",
    "random.shuffle(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our datasets now properly set up, we'll now create `Example` objects to feed into the training process. This object is new in spaCy v3. Essentially, it contains a document with predictions (`predicted`) and one with gold-standard annotations (`reference`). During training, the pipeline will compare its predictions to the gold-standard, and update the weights of the neural network accordingly.\n",
    "\n",
    "For entity linking, the algorithm needs access to gold-standard sentences, because the algorithms use the context from the sentence to perform the disambiguation. You can either provide gold-standard `sent_starts` annotations, or run a component such as the `parser` or `sentencizer` on your reference documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.training import Example\n",
    "\n",
    "TRAIN_EXAMPLES = []\n",
    "if \"sentencizer\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "sentencizer = nlp.get_pipe(\"sentencizer\")\n",
    "for text, annotation in train_dataset:\n",
    "    example = Example.from_dict(nlp.make_doc(text), annotation)\n",
    "    example.reference = sentencizer(example.reference)\n",
    "    TRAIN_EXAMPLES.append(example)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll create a new Entity Linking component and add it to the pipeline. \n",
    "\n",
    "We also need to make sure the `entity_linker` component is properly initialized. To do this, we need a `get_examples` function that returns some example training data, as well as a `kb_loader` argument. This is a `Callable` function that creates the `KnowledgeBase`, given a certain `Vocab` instance. Here, we will load our KB from disk, using the built-in [`spacy.KBFromFile.v1`](https://spacy.io/api/architectures#KBFromFile) function, which is defined in `spacy.ml.models`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.ml.models import load_kb\n",
    "\n",
    "entity_linker = nlp.add_pipe(\"entity_linker\", config={\"incl_prior\": False}, last=True)\n",
    "entity_linker.initialize(get_examples=lambda: TRAIN_EXAMPLES, kb_loader=load_kb(output_dir / \"my_kb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will run the actual training loop for the new component, taking care to only train the entity linker and not the other components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "with nlp.select_pipes(enable=[\"entity_linker\"]):   # train only the entity_linker\n",
    "    optimizer = nlp.resume_training()\n",
    "    for itn in range(500):   # 500 iterations takes about a minute to train\n",
    "        random.shuffle(TRAIN_EXAMPLES)\n",
    "        batches = minibatch(TRAIN_EXAMPLES, size=compounding(4.0, 32.0, 1.001))  # increasing batch sizes\n",
    "        losses = {}\n",
    "        for batch in batches:\n",
    "            nlp.update(\n",
    "                batch,   \n",
    "                drop=0.2,      # prevent overfitting\n",
    "                losses=losses,\n",
    "                sgd=optimizer,\n",
    "            )\n",
    "        if itn % 50 == 0:\n",
    "            print(itn, \"Losses\", losses)   # print the training loss\n",
    "print(itn, \"Losses\", losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final training loss is pretty small, which is a good sign. But to truly verify whether our model generalizes well, we need to test it on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Entity Linker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first apply it on our original sentence. For each entity, we print the text and label as before, but also the disambiguated QID as predicted by our entity linker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Tennis champion Emerson was expected to win Wimbledon.\"\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_, ent.kb_id_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Emerson gets disambiguated to Q312545, which is the correct ID for the tennis player. Note also that the entity \"Wimbledon\" gets the annotation `NIL`, which is basically just a placeholder value, showing that the NEL component could not find any relevant ID for this entity. This happens because our Knowledge base and the Entity Linking component have only been trained on \"Emerson\" examples, and are thus quite limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the model predicts for the 6 sentences in our test dataset, that were never seen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, true_annot in test_dataset:\n",
    "    print(text)\n",
    "    print(f\"Gold annotation: {true_annot}\")\n",
    "    doc = nlp(text)  # to make this more efficient, you can use nlp.pipe() just once for all the texts\n",
    "    for ent in doc.ents:\n",
    "        if ent.text == \"Emerson\":\n",
    "            print(f\"Prediction: {ent.text}, {ent.label_}, {ent.kb_id_}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results may vary a little from run to run, but usually the EL pipeline will get 5 out of 6 predictions correct (83% accuracy). Random guessing would have only achieved 33%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, this tutorial has shown you how to implement an Entity Linking component in spaCy. The knowledge base and training dataset used here were kept small for demonstration purposes, but in reality you'll want to use a much bigger representative set of entities, perhaps from an ontology or dictionary that is relevant to your use-case. \n",
    "\n",
    "If you have general questions on how to use this functionality in your own application, the best route is to create a new StackOverfow issue and tag it with the label `spaCy`. If you would run into an actual bug with the Entity Linking functionality, you can also open an issue at spaCy's github tracker. \n",
    "\n",
    "I hope your next NLP project will incorporate entity linking !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Entity Linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entity Linking** (EL) is the challenge of resolving ambiguous textual mentions to unique concepts in a knowledge base. A related task is **Named Entity Recognition** (NER). An NER component basically identifies words in text that have a specific name and refer to real-world objects, such as people or organizations. spaCy offers pre-built Machine Learning models that perform Named Entity Recognition for a variety of languages (https://spacy.io/models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "!pip install spacy==3.0.6\n",
    "!pip install spacy-lookups-data\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luis_flores/.local/share/virtualenvs/ner-DhZLIlym/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from gu_model.trf_tensor_to_vec import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luis_flores/.local/share/virtualenvs/ner-DhZLIlym/lib/python3.7/site-packages/spacy/util.py:833: UserWarning: [W095] Model 'en_ner_guardian' (1.0.3) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.2.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/luis_flores/.local/share/virtualenvs/ner-DhZLIlym/lib/python3.7/site-packages/spacy_transformers/pipeline_component.py:406: UserWarning: Automatically converting a transformer component from spacy-transformers v1.0 to v1.1+. If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spacy-transformers version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "## Load spacy pipeline\n",
    "\n",
    "#nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp = spacy.load('gu_model/en_ner_guardian-1.0.3/en_ner_guardian/en_ner_guardian-1.0.3',\n",
    "                     disable=['transformer', 'tagger', 'parser', 'lemmatizer', 'attribute_ruler'])\n",
    "nlp.add_pipe('tensor2attr')\n",
    "\n",
    "text = \"Tennis champion Emerson was expected to win Wimbledon.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Find nlp model embedding dimensions\n",
    "embedding_dims=len(doc.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entity 'Emerson' with label 'PERSON'\n"
     ]
    }
   ],
   "source": [
    "# Test spacy model\n",
    "for ent in doc.ents:\n",
    "    print(f\"Named Entity '{ent.text}' with label '{ent.label_}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Knowledge Base "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to perform Entity Linking, is to set up a knowledge base that contains the unique identifiers of the entities we are interested in. In this tutorial we will create a very simple one with only 3 entries. We load the data from a pre-defined CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='full'# OR'open_sanctions'# OR 'lilsis'\n",
    "kb_iteration='_2022_11_03'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import kb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luis_flores/.local/share/virtualenvs/ner-DhZLIlym/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3553: DtypeWarning: Columns (2,7,11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv(f'kb_datasets/kb_entities_{dataset}{kb_iteration}.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_index</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>AKA</th>\n",
       "      <th>birthdate</th>\n",
       "      <th>deathdate</th>\n",
       "      <th>wikidataId</th>\n",
       "      <th>website</th>\n",
       "      <th>desc</th>\n",
       "      <th>kb_origin</th>\n",
       "      <th>birthplace</th>\n",
       "      <th>desc_len</th>\n",
       "      <th>kb_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>acf-00040861bc3f593000830d987d09967ef3503ef1</td>\n",
       "      <td>Kolyvanov Egor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1980-11-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kolyvanov Egor is a Russian propagandist: host...</td>\n",
       "      <td>open_sanctions</td>\n",
       "      <td>NaN</td>\n",
       "      <td>228</td>\n",
       "      <td>https://www.opensanctions.org/entities/acf-000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>acf-0011c68a768924609dc5da5707ac7fa4c4d645a2</td>\n",
       "      <td>Shipov Sergei Yurievich</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1966-04-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shipov Sergei Yurievich is a Russian chess pla...</td>\n",
       "      <td>open_sanctions</td>\n",
       "      <td>NaN</td>\n",
       "      <td>258</td>\n",
       "      <td>https://www.opensanctions.org/entities/acf-001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>acf-001e7e4c0363f08f1e784c230457960b84a6416f</td>\n",
       "      <td>Egorov Ivan Mikhailovich</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1961-01-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Egorov Ivan Mikhailovich is a Deputy of the St...</td>\n",
       "      <td>open_sanctions</td>\n",
       "      <td>NaN</td>\n",
       "      <td>344</td>\n",
       "      <td>https://www.opensanctions.org/entities/acf-001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>acf-002c208139012c8d93b6298358188d7cadafe648</td>\n",
       "      <td>Goreslavsky Alexey Sergeyevich</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1977-07-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Goreslavsky Alexey Sergeyevich is a Russian jo...</td>\n",
       "      <td>open_sanctions</td>\n",
       "      <td>NaN</td>\n",
       "      <td>773</td>\n",
       "      <td>https://www.opensanctions.org/entities/acf-002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>acf-002cc8fdf8fe41185091a7cb6c598663e7a22eb5</td>\n",
       "      <td>Samoilova Natalya Vladimirovna</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1987-06-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Samoilova Natalya Vladimirovna is a Russian si...</td>\n",
       "      <td>open_sanctions</td>\n",
       "      <td>NaN</td>\n",
       "      <td>302</td>\n",
       "      <td>https://www.opensanctions.org/entities/acf-002...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original_index                                            id  \\\n",
       "0               0  acf-00040861bc3f593000830d987d09967ef3503ef1   \n",
       "1               1  acf-0011c68a768924609dc5da5707ac7fa4c4d645a2   \n",
       "2               2  acf-001e7e4c0363f08f1e784c230457960b84a6416f   \n",
       "3               3  acf-002c208139012c8d93b6298358188d7cadafe648   \n",
       "4               4  acf-002cc8fdf8fe41185091a7cb6c598663e7a22eb5   \n",
       "\n",
       "                             name  AKA   birthdate deathdate wikidataId  \\\n",
       "0                  Kolyvanov Egor  NaN  1980-11-15       NaN        NaN   \n",
       "1         Shipov Sergei Yurievich  NaN  1966-04-17       NaN        NaN   \n",
       "2        Egorov Ivan Mikhailovich  NaN  1961-01-21       NaN        NaN   \n",
       "3  Goreslavsky Alexey Sergeyevich  NaN  1977-07-13       NaN        NaN   \n",
       "4  Samoilova Natalya Vladimirovna  NaN  1987-06-24       NaN        NaN   \n",
       "\n",
       "  website                                               desc       kb_origin  \\\n",
       "0     NaN  Kolyvanov Egor is a Russian propagandist: host...  open_sanctions   \n",
       "1     NaN  Shipov Sergei Yurievich is a Russian chess pla...  open_sanctions   \n",
       "2     NaN  Egorov Ivan Mikhailovich is a Deputy of the St...  open_sanctions   \n",
       "3     NaN  Goreslavsky Alexey Sergeyevich is a Russian jo...  open_sanctions   \n",
       "4     NaN  Samoilova Natalya Vladimirovna is a Russian si...  open_sanctions   \n",
       "\n",
       "  birthplace  desc_len                                             kb_url  \n",
       "0        NaN       228  https://www.opensanctions.org/entities/acf-000...  \n",
       "1        NaN       258  https://www.opensanctions.org/entities/acf-001...  \n",
       "2        NaN       344  https://www.opensanctions.org/entities/acf-001...  \n",
       "3        NaN       773  https://www.opensanctions.org/entities/acf-002...  \n",
       "4        NaN       302  https://www.opensanctions.org/entities/acf-002...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_data=data[['id','name','desc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acf-00040861bc3f593000830d987d09967ef3503ef1</td>\n",
       "      <td>Kolyvanov Egor</td>\n",
       "      <td>Kolyvanov Egor is a Russian propagandist: host...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acf-0011c68a768924609dc5da5707ac7fa4c4d645a2</td>\n",
       "      <td>Shipov Sergei Yurievich</td>\n",
       "      <td>Shipov Sergei Yurievich is a Russian chess pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acf-001e7e4c0363f08f1e784c230457960b84a6416f</td>\n",
       "      <td>Egorov Ivan Mikhailovich</td>\n",
       "      <td>Egorov Ivan Mikhailovich is a Deputy of the St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acf-002c208139012c8d93b6298358188d7cadafe648</td>\n",
       "      <td>Goreslavsky Alexey Sergeyevich</td>\n",
       "      <td>Goreslavsky Alexey Sergeyevich is a Russian jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acf-002cc8fdf8fe41185091a7cb6c598663e7a22eb5</td>\n",
       "      <td>Samoilova Natalya Vladimirovna</td>\n",
       "      <td>Samoilova Natalya Vladimirovna is a Russian si...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             id  \\\n",
       "0  acf-00040861bc3f593000830d987d09967ef3503ef1   \n",
       "1  acf-0011c68a768924609dc5da5707ac7fa4c4d645a2   \n",
       "2  acf-001e7e4c0363f08f1e784c230457960b84a6416f   \n",
       "3  acf-002c208139012c8d93b6298358188d7cadafe648   \n",
       "4  acf-002cc8fdf8fe41185091a7cb6c598663e7a22eb5   \n",
       "\n",
       "                             name  \\\n",
       "0                  Kolyvanov Egor   \n",
       "1         Shipov Sergei Yurievich   \n",
       "2        Egorov Ivan Mikhailovich   \n",
       "3  Goreslavsky Alexey Sergeyevich   \n",
       "4  Samoilova Natalya Vladimirovna   \n",
       "\n",
       "                                                desc  \n",
       "0  Kolyvanov Egor is a Russian propagandist: host...  \n",
       "1  Shipov Sergei Yurievich is a Russian chess pla...  \n",
       "2  Egorov Ivan Mikhailovich is a Deputy of the St...  \n",
       "3  Goreslavsky Alexey Sergeyevich is a Russian jo...  \n",
       "4  Samoilova Natalya Vladimirovna is a Russian si...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(428519, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate synthetic aliases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "aliases_data=kb_data[kb_data['name'].duplicated(keep=False)].sort_values(['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "aliases_data['id']=aliases_data['id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "alias_dict={}\n",
    "for alias in aliases_data['name'].unique():\n",
    "    alias_dict[alias]=list(aliases_data.loc[aliases_data['name']==alias, 'id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "David Smith                 13\n",
       "Mark Smith                  13\n",
       "David Wilson                13\n",
       "John Williams               12\n",
       "Robert Smith                12\n",
       "                            ..\n",
       "Véronique Albanel            1\n",
       "David Weytsman               1\n",
       "Hiroshi Oka                  1\n",
       "Andrew Cray                  1\n",
       "Jalbasürengiin Batzandan     1\n",
       "Name: name, Length: 414002, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb_data['name'].value_counts()#.to_csv('value_counts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>name</th>\n",
       "      <th>desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acf-00040861bc3f593000830d987d09967ef3503ef1</td>\n",
       "      <td>Kolyvanov Egor</td>\n",
       "      <td>Kolyvanov Egor is a Russian propagandist: host...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acf-0011c68a768924609dc5da5707ac7fa4c4d645a2</td>\n",
       "      <td>Shipov Sergei Yurievich</td>\n",
       "      <td>Shipov Sergei Yurievich is a Russian chess pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acf-001e7e4c0363f08f1e784c230457960b84a6416f</td>\n",
       "      <td>Egorov Ivan Mikhailovich</td>\n",
       "      <td>Egorov Ivan Mikhailovich is a Deputy of the St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acf-002c208139012c8d93b6298358188d7cadafe648</td>\n",
       "      <td>Goreslavsky Alexey Sergeyevich</td>\n",
       "      <td>Goreslavsky Alexey Sergeyevich is a Russian jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acf-002cc8fdf8fe41185091a7cb6c598663e7a22eb5</td>\n",
       "      <td>Samoilova Natalya Vladimirovna</td>\n",
       "      <td>Samoilova Natalya Vladimirovna is a Russian si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428514</th>\n",
       "      <td>Q4354299</td>\n",
       "      <td>Cory Bernardi</td>\n",
       "      <td>Cory Bernardi is a Australian politician and r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428515</th>\n",
       "      <td>Q47668202</td>\n",
       "      <td>Jalbasürengiin Batzandan</td>\n",
       "      <td>Jalbasürengiin Batzandan is a Mongolian politi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428516</th>\n",
       "      <td>Q5997832</td>\n",
       "      <td>Patrick Murphy</td>\n",
       "      <td>Patrick Murphy is a former US Representative f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428517</th>\n",
       "      <td>Q28033808</td>\n",
       "      <td>Sharif Street</td>\n",
       "      <td>Sharif Street is a American politician from Pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428518</th>\n",
       "      <td>Q44853588</td>\n",
       "      <td>Wendy Carrillo</td>\n",
       "      <td>Wendy Carrillo is a Assemblymember representin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428519 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 qid  \\\n",
       "0       acf-00040861bc3f593000830d987d09967ef3503ef1   \n",
       "1       acf-0011c68a768924609dc5da5707ac7fa4c4d645a2   \n",
       "2       acf-001e7e4c0363f08f1e784c230457960b84a6416f   \n",
       "3       acf-002c208139012c8d93b6298358188d7cadafe648   \n",
       "4       acf-002cc8fdf8fe41185091a7cb6c598663e7a22eb5   \n",
       "...                                              ...   \n",
       "428514                                      Q4354299   \n",
       "428515                                     Q47668202   \n",
       "428516                                      Q5997832   \n",
       "428517                                     Q28033808   \n",
       "428518                                     Q44853588   \n",
       "\n",
       "                                  name  \\\n",
       "0                       Kolyvanov Egor   \n",
       "1              Shipov Sergei Yurievich   \n",
       "2             Egorov Ivan Mikhailovich   \n",
       "3       Goreslavsky Alexey Sergeyevich   \n",
       "4       Samoilova Natalya Vladimirovna   \n",
       "...                                ...   \n",
       "428514                   Cory Bernardi   \n",
       "428515        Jalbasürengiin Batzandan   \n",
       "428516                  Patrick Murphy   \n",
       "428517                   Sharif Street   \n",
       "428518                  Wendy Carrillo   \n",
       "\n",
       "                                                     desc  \n",
       "0       Kolyvanov Egor is a Russian propagandist: host...  \n",
       "1       Shipov Sergei Yurievich is a Russian chess pla...  \n",
       "2       Egorov Ivan Mikhailovich is a Deputy of the St...  \n",
       "3       Goreslavsky Alexey Sergeyevich is a Russian jo...  \n",
       "4       Samoilova Natalya Vladimirovna is a Russian si...  \n",
       "...                                                   ...  \n",
       "428514  Cory Bernardi is a Australian politician and r...  \n",
       "428515  Jalbasürengiin Batzandan is a Mongolian politi...  \n",
       "428516  Patrick Murphy is a former US Representative f...  \n",
       "428517  Sharif Street is a American politician from Pe...  \n",
       "428518  Wendy Carrillo is a Assemblymember representin...  \n",
       "\n",
       "[428519 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export kb data in right format for tutorialkb_data\n",
    "kb_data.rename(columns={'id':'qid','context':'desc'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ed Williams is a Prospective Parliamentary Candidate for Meriden.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(kb_data.loc[kb_data['name']=='Ed Williams','desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csv\n",
    "from pathlib import Path\n",
    "\n",
    "def load_entities(kb_data):\n",
    "    names = dict()\n",
    "    descriptions = dict()\n",
    "\n",
    "    for row in kb_data.iterrows():\n",
    "        qid = str(row[1][0])\n",
    "        name = str(row[1][1])\n",
    "        desc = str(row[1][2])\n",
    "        names[qid] = name\n",
    "        descriptions[qid] = desc\n",
    "    \n",
    "    return names, descriptions\n",
    "\n",
    "# Call function\n",
    "name_dict, desc_dict = load_entities(kb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.kb import KnowledgeBase\n",
    "\n",
    "kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=embedding_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add each record to the knowledge base, we encode its description using the built-in word vectors of our `nlp` model. The `vector` attribute of a document is the average of its token vectors. We also need to provide a frequency, which is a raw count of how many times a certain entity appears in an annotated corpus. In this tutorial we're not using these frequencies, so we're setting them to an arbitrary value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "descriptions_enc = dict()\n",
    "for qid, desc in desc_dict.items():\n",
    "    #desc_doc = nlp(desc)\n",
    "    #desc_enc = desc_doc.vector\n",
    "    desc_enc=np.zeros(embedding_dims)\n",
    "    #descriptions_enc[qid]=desc_enc\n",
    "    kb.add_entity(entity=qid, entity_vector=desc_enc, freq=342)   # 342 is an arbitrary value here"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pickle\n",
    "\n",
    "with open('kb_datasets/descriptions_enc.pickle', 'wb') as handle:\n",
    "    pickle.dump(descriptions_enc, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to specify aliases or synonyms. We first add the full names. Here, we are 100% certain that they resolve to their corresponding QID, as there is no ambiguity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qid, name in name_dict.items():\n",
    "    if name not in alias_dict.keys():\n",
    "        kb.add_alias(alias=str(name), entities=[str(qid)], probabilities=[1])   # 100% prior probability P(entity|alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alias_ in alias_dict.keys():\n",
    "    qids=alias_dict[alias_]\n",
    "    probs = [round(1/len(qids),2)-.01 for qid in qids]\n",
    "    kb.add_alias(alias=alias_, entities=qids, probabilities=probs)  # sum([probs]) should be <= 1 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to add the alias \"Emerson\". We'll assume that each of our 3 Emersons is equally famous and thus we set their probabilities to be equal for each entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this will be our Knowledge base. We can check the entities and aliases that are contained in it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the candidates that are generated for the full name of Roy Emerson, as well as for the mention \"Emerson\" or for any other random mention, like \"Sofie\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates for Joe Biden: ['Q6279']\n"
     ]
    }
   ],
   "source": [
    "candidate_1='Joe Biden'\n",
    "print(f\"Candidates for {candidate_1}: {[c.entity_ for c in kb.get_alias_candidates(candidate_1)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates for Adam Smith: ['129552', '379819', '269916', '256328', 'Q350916']\n"
     ]
    }
   ],
   "source": [
    "candidate_2='Adam Smith'\n",
    "print(f\"Candidates for {candidate_2}: {[c.entity_ for c in kb.get_alias_candidates(candidate_2)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates for David Smith: ['280783', '211703', '53881', '200407', '377020', '204251', '184041', '77215', 'Q3018800', '221595', 'Q53960880', 'Q5239878', '200405']\n"
     ]
    }
   ],
   "source": [
    "candidate_3='David Smith'\n",
    "print(f\"Candidates for {candidate_3}: {[c.entity_ for c in kb.get_alias_candidates(candidate_3)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that querying the KB with the alias \"Emerson\" gives us 3 candidates, but if we query it with an unknown term, it just gives an empty list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the knowledge base by calling the function `to_disk` with an output location."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "f = open(\"source_data.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "source_data_sentences=[item + ' \\n' for sublist in [article.split('.') for article in list(f)] for item in sublist] "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(source_data_sentences)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('source_data_sentences.txt', 'w') as fp:\n",
    "    for item in source_data_sentences:\n",
    "        # write each item on a new line\n",
    "        fp.write(item)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='full'\n",
    "dataset=f'{dataset}{kb_iteration}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# change the directory and file names to whatever you like\n",
    "import os\n",
    "output_dir = Path.cwd() / \"assets\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir) \n",
    "kb.to_disk(output_dir / f'kb_{dataset}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can store the `nlp` object to file by calling `to_disk` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(output_dir / f'nlp_{dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125089</th>\n",
       "      <td>Q3018800</td>\n",
       "      <td>David Smith</td>\n",
       "      <td>David Smith is a Quebec politician. This perso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149306</th>\n",
       "      <td>Q5239878</td>\n",
       "      <td>David Smith</td>\n",
       "      <td>David Smith is a Canadian senator. This person...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150651</th>\n",
       "      <td>Q53960880</td>\n",
       "      <td>David Smith</td>\n",
       "      <td>David Smith is a Australian Capital Territory ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249323</th>\n",
       "      <td>53881</td>\n",
       "      <td>David Smith</td>\n",
       "      <td>David Smith is a Prospective Parliamentary Can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265392</th>\n",
       "      <td>77215</td>\n",
       "      <td>David Smith</td>\n",
       "      <td>David Smith is a Dasa Properties LLC .     .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325350</th>\n",
       "      <td>184041</td>\n",
       "      <td>David Smith</td>\n",
       "      <td>David Smith is a Welder, Sun Coast Resources I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336129</th>\n",
       "      <td>200405</td>\n",
       "      <td>David Smith</td>\n",
       "      <td>David Smith is a Professor, University of Flor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336131</th>\n",
       "      <td>200407</td>\n",
       "      <td>David Smith</td>\n",
       "      <td>David Smith is a Professor, San Bernardino Com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338410</th>\n",
       "      <td>204251</td>\n",
       "      <td>David Smith</td>\n",
       "      <td>David Smith is a San Bernardino Community Coll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342983</th>\n",
       "      <td>211703</td>\n",
       "      <td>David Smith</td>\n",
       "      <td>David Smith is a Property Management Administr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348660</th>\n",
       "      <td>221595</td>\n",
       "      <td>David Smith</td>\n",
       "      <td>David Smith is a Regional Vice President, Anth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375169</th>\n",
       "      <td>280783</td>\n",
       "      <td>David Smith</td>\n",
       "      <td>David Smith is a Chief Development Officer, Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409585</th>\n",
       "      <td>377020</td>\n",
       "      <td>David Smith</td>\n",
       "      <td>David Smith is a PNC Bank.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id         name  \\\n",
       "125089   Q3018800  David Smith   \n",
       "149306   Q5239878  David Smith   \n",
       "150651  Q53960880  David Smith   \n",
       "249323      53881  David Smith   \n",
       "265392      77215  David Smith   \n",
       "325350     184041  David Smith   \n",
       "336129     200405  David Smith   \n",
       "336131     200407  David Smith   \n",
       "338410     204251  David Smith   \n",
       "342983     211703  David Smith   \n",
       "348660     221595  David Smith   \n",
       "375169     280783  David Smith   \n",
       "409585     377020  David Smith   \n",
       "\n",
       "                                                     desc  \n",
       "125089  David Smith is a Quebec politician. This perso...  \n",
       "149306  David Smith is a Canadian senator. This person...  \n",
       "150651  David Smith is a Australian Capital Territory ...  \n",
       "249323  David Smith is a Prospective Parliamentary Can...  \n",
       "265392       David Smith is a Dasa Properties LLC .     .  \n",
       "325350  David Smith is a Welder, Sun Coast Resources I...  \n",
       "336129  David Smith is a Professor, University of Flor...  \n",
       "336131  David Smith is a Professor, San Bernardino Com...  \n",
       "338410  David Smith is a San Bernardino Community Coll...  \n",
       "342983  David Smith is a Property Management Administr...  \n",
       "348660  David Smith is a Regional Vice President, Anth...  \n",
       "375169  David Smith is a Chief Development Officer, Le...  \n",
       "409585                         David Smith is a PNC Bank.  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb_data[kb_data['name']=='David Smith']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to create some annotated data to train an Entity Linking algorithm on. To do so, we will use the annotation tool Prodigy, but you could generate the data in whatever tool you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are watching [the video](https://www.youtube.com/watch?v=8u57WSXVpmw), it will explain how to obtain annotated data with Prodigy. The final result will be a JSONL file that is distributed alongside this notebook. We'll now use this JSONL file to train our entity linker. If you want to skip the annotation part in the video, you can fast forward to [this section](https://www.youtube.com/watch?v=8u57WSXVpmw&t=19m19s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the results in this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "json_loc = Path.cwd().parent / \"assets\" / \"emerson_annotated_text.jsonl\" # distributed alongside this notebook\n",
    "with json_loc.open(\"r\", encoding=\"utf8\") as jsonfile:\n",
    "    line = jsonfile.readline()\n",
    "    print(line)   # print just the first line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We see that the full text of the original sentence is stored, together with a lot of detail about the annotation task. The most important bit is stored with the key `accept` at the end: this is the value of our manual annotation. For this specific sentence and this specific mention, the option with key `Q312545` was manually selected. This is the information that we'll train our entity linker on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Entity Linker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed training data into our Entity Linker, we format our data as a structured tuple. The first part is the raw text, and the second part is a dictionary of annotations. This dictionary defines the named entities we want to link (\"entities\"), as well as the actual gold-standard links (\"links\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "dataset = []\n",
    "json_loc = Path.cwd().parent / \"assets\" / \"emerson_annotated_text.jsonl\"\n",
    "with json_loc.open(\"r\", encoding=\"utf8\") as jsonfile:\n",
    "    for line in jsonfile:\n",
    "        example = json.loads(line)\n",
    "        text = example[\"text\"]\n",
    "        if example[\"answer\"] == \"accept\":\n",
    "            QID = example[\"accept\"][0]\n",
    "            offset = (example[\"spans\"][0][\"start\"], example[\"spans\"][0][\"end\"])\n",
    "            entity_label = example[\"spans\"][0][\"label\"]\n",
    "            entities = [(offset[0], offset[1], entity_label)]\n",
    "            links_dict = {QID: 1.0}\n",
    "        dataset.append((text, {\"links\": {offset: links_dict}, \"entities\": entities}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check whether the conversion looks OK, we can just print the first sample in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check some statistics in this dataset. How many cases of each QID do we have annotated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_ids = []\n",
    "for text, annot in dataset:\n",
    "    for span, links_dict in annot[\"links\"].items():\n",
    "        for link, value in links_dict.items():\n",
    "            if value:\n",
    "                gold_ids.append(link)\n",
    "\n",
    "from collections import Counter\n",
    "print(Counter(gold_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got exactly 10 annotated sentences for each of our Emersons. Of these, we'll now set aside 6 cases in a separate test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_dataset = []\n",
    "test_dataset = []\n",
    "for QID in qids:\n",
    "    indices = [i for i, j in enumerate(gold_ids) if j == QID]\n",
    "    train_dataset.extend(dataset[index] for index in indices[0:8])  # first 8 in training\n",
    "    test_dataset.extend(dataset[index] for index in indices[8:10])  # last 2 in test\n",
    "    \n",
    "random.shuffle(train_dataset)\n",
    "random.shuffle(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our datasets now properly set up, we'll now create `Example` objects to feed into the training process. This object is new in spaCy v3. Essentially, it contains a document with predictions (`predicted`) and one with gold-standard annotations (`reference`). During training, the pipeline will compare its predictions to the gold-standard, and update the weights of the neural network accordingly.\n",
    "\n",
    "For entity linking, the algorithm needs access to gold-standard sentences, because the algorithms use the context from the sentence to perform the disambiguation. You can either provide gold-standard `sent_starts` annotations, or run a component such as the `parser` or `sentencizer` on your reference documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.training import Example\n",
    "\n",
    "TRAIN_EXAMPLES = []\n",
    "if \"sentencizer\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "sentencizer = nlp.get_pipe(\"sentencizer\")\n",
    "for text, annotation in train_dataset:\n",
    "    example = Example.from_dict(nlp.make_doc(text), annotation)\n",
    "    example.reference = sentencizer(example.reference)\n",
    "    TRAIN_EXAMPLES.append(example)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll create a new Entity Linking component and add it to the pipeline. \n",
    "\n",
    "We also need to make sure the `entity_linker` component is properly initialized. To do this, we need a `get_examples` function that returns some example training data, as well as a `kb_loader` argument. This is a `Callable` function that creates the `KnowledgeBase`, given a certain `Vocab` instance. Here, we will load our KB from disk, using the built-in [`spacy.KBFromFile.v1`](https://spacy.io/api/architectures#KBFromFile) function, which is defined in `spacy.ml.models`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.ml.models import load_kb\n",
    "\n",
    "entity_linker = nlp.add_pipe(\"entity_linker\", config={\"incl_prior\": False}, last=True)\n",
    "entity_linker.initialize(get_examples=lambda: TRAIN_EXAMPLES, kb_loader=load_kb(output_dir / \"my_kb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will run the actual training loop for the new component, taking care to only train the entity linker and not the other components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "with nlp.select_pipes(enable=[\"entity_linker\"]):   # train only the entity_linker\n",
    "    optimizer = nlp.resume_training()\n",
    "    for itn in range(500):   # 500 iterations takes about a minute to train\n",
    "        random.shuffle(TRAIN_EXAMPLES)\n",
    "        batches = minibatch(TRAIN_EXAMPLES, size=compounding(4.0, 32.0, 1.001))  # increasing batch sizes\n",
    "        losses = {}\n",
    "        for batch in batches:\n",
    "            nlp.update(\n",
    "                batch,   \n",
    "                drop=0.2,      # prevent overfitting\n",
    "                losses=losses,\n",
    "                sgd=optimizer,\n",
    "            )\n",
    "        if itn % 50 == 0:\n",
    "            print(itn, \"Losses\", losses)   # print the training loss\n",
    "print(itn, \"Losses\", losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final training loss is pretty small, which is a good sign. But to truly verify whether our model generalizes well, we need to test it on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Entity Linker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first apply it on our original sentence. For each entity, we print the text and label as before, but also the disambiguated QID as predicted by our entity linker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Tennis champion Emerson was expected to win Wimbledon.\"\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_, ent.kb_id_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Emerson gets disambiguated to Q312545, which is the correct ID for the tennis player. Note also that the entity \"Wimbledon\" gets the annotation `NIL`, which is basically just a placeholder value, showing that the NEL component could not find any relevant ID for this entity. This happens because our Knowledge base and the Entity Linking component have only been trained on \"Emerson\" examples, and are thus quite limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the model predicts for the 6 sentences in our test dataset, that were never seen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, true_annot in test_dataset:\n",
    "    print(text)\n",
    "    print(f\"Gold annotation: {true_annot}\")\n",
    "    doc = nlp(text)  # to make this more efficient, you can use nlp.pipe() just once for all the texts\n",
    "    for ent in doc.ents:\n",
    "        if ent.text == \"Emerson\":\n",
    "            print(f\"Prediction: {ent.text}, {ent.label_}, {ent.kb_id_}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results may vary a little from run to run, but usually the EL pipeline will get 5 out of 6 predictions correct (83% accuracy). Random guessing would have only achieved 33%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, this tutorial has shown you how to implement an Entity Linking component in spaCy. The knowledge base and training dataset used here were kept small for demonstration purposes, but in reality you'll want to use a much bigger representative set of entities, perhaps from an ontology or dictionary that is relevant to your use-case. \n",
    "\n",
    "If you have general questions on how to use this functionality in your own application, the best route is to create a new StackOverfow issue and tag it with the label `spaCy`. If you would run into an actual bug with the Entity Linking functionality, you can also open an issue at spaCy's github tracker. \n",
    "\n",
    "I hope your next NLP project will incorporate entity linking !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
